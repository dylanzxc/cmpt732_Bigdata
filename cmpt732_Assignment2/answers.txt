Question 1First, we should define a class maybe named “LongTextPairWritable” to store the (view count, page title) just like the LongPairWritable we defined in Assignment 1 except here the page title is not a long. Then in the Mapper class we should change the output format to <Text ,LongTextPairWritable>  and store the [2]  and [4] elements of the String[] which store the result of whitespace.split(input_string.toString()). Write the (Text, LongTextPairWritable) result to the context. In the reducer, instead of finding the max_view we should find the max_pair using the .get_0() and .get_1() method and return the <Text ,LongTextPairWritable>  at the end.As always, we need to modify the main method with the new class name and input output format.Question 2Using wordcount as an example. text = sc.textFile(inputs) creates an RDD called text with each line as a string text=[”This is a string”“Hello World”]words = text.flatMap(words_once) flatmap apply words_once on each line of the RDD and create a new RDD with tuples like (This,1)(is,1)(a,1)So from some point of view the .flatMap() method can iterate through the whole input file which is very like the MapReduce concept of mapping.However, .map() method in Spark simply means apply the function in () to the RDD. There is no iteration.Question 3No they are not the same. reduce must pull the entire dataset down into a single location because it is reducing to one final value. reduceByKey on the other hand is one value for each key. And since this action can be run on each machine locally first then it can remain an RDD and have further transformations done on its dataset.The .reduce is called an action and the .reduceByKey is called a transformation.Question 41) When two different pages in the same time period have the same view counts, we want to return the count views and also the different titles corresponding to that maximum view counts, for example (20160801-000000, (146, ‘Simon_Pegg’, ‘Mary’)). To do so we have to modify our get_max function. The reduceByKey method will hold all the (viewcounts,title) tuples with the same key together and made it to a list. Our previous get_max function will iterate through all the tuples and return the maximized one (compare first two tuples and return the max and repeat this process). Now instead of returning the max tuple directly, we can find the largest view count first and .append() all the titles with the largest view count to a new list and return that list with the key. I have wrote a code to  implement this:def get_max(a,b):    highest=max(a[0],b[0])     max_list=[]    if a[0]==highest:        max_list.append(a)    elif b[0]==highest:        max_list.append(b[1])    return max_list

Or

def get_max(a,b):
    max_list=[]
    if a[0]==b[0]:
        max_list.append(a)
        max_list.append(b)
    else:
        max_list.append(max(a,b))   
    return max_list   
    2)To find all of the page views the maximum number of times, we can use the RDD.max() function or the Array.maxBy to find the maximum item in this RDD